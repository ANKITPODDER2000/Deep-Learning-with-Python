{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 27s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data , train_label) , (test_data , test_label) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data[0] :=  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "\n",
      "train_label[0] :=  1\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data[0] := \",train_data[0])\n",
    "print(\"\\n\\ntrain_label[0] := \",train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = max([max(inp) for inp in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max in train_data :=  9999\n"
     ]
    }
   ],
   "source": [
    "print(\"Max in train_data := \",max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 4s 3us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_word_index = {}\n",
    "for k,v in word_index.items():\n",
    "    rev_word_index[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(L):\n",
    "    s = \"\"\n",
    "    for i in L:\n",
    "        s += rev_word_index[i] +\" \"\n",
    "    s = s[:-1] + \".\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the effort still been that usually makes for of finished sucking ended and an because before if just though something know novel female i i slowly lot of above and with connect in of script their that out end his and i i.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_to_str(train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = [\"I am Ankit.\",\"I am Ankita paul\"]\n",
    "tokenizer = Tokenizer(oov_token=\"<oov>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"I am raja\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq = tokenizer.texts_to_sequences(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 3, 5, 6, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(Seq,maxlen=10,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "for inp in train_data:\n",
    "    m = max(m , len(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncating = \"post\"\n",
    "padding    = \"post\"\n",
    "maxlen     = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_data,maxlen=maxlen,padding=padding,truncating=truncating)\n",
    "X_test = pad_sequences(test_data,maxlen=maxlen,padding=padding,truncating=truncating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_label,dtype=np.float64)\n",
    "y_test  = np.array(test_label,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 120)\n",
      "(25000, 120)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape )\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(10000 , 32 , input_length = maxlen),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16 , activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(16 , activation = \"relu\"),\n",
    "    tf.keras.layers.Dense( 1 , activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 32)           320000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,817\n",
      "Trainable params: 320,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.RMSprop(lr = 0.001) ,\n",
    "               loss = tf.keras.losses.binary_crossentropy ,\n",
    "               metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 2s 68us/sample - loss: 0.2402 - acc: 0.9073 - val_loss: 0.3840 - val_acc: 0.8368\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 65us/sample - loss: 0.2237 - acc: 0.9151 - val_loss: 0.4057 - val_acc: 0.8307\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.2103 - acc: 0.9236 - val_loss: 0.4219 - val_acc: 0.8274\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.1984 - acc: 0.9286 - val_loss: 0.4284 - val_acc: 0.8276\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.1877 - acc: 0.9342 - val_loss: 0.4405 - val_acc: 0.8274\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s 64us/sample - loss: 0.1794 - acc: 0.9371 - val_loss: 0.4553 - val_acc: 0.8252\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.1708 - acc: 0.9408 - val_loss: 0.4983 - val_acc: 0.8168\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.1636 - acc: 0.9444 - val_loss: 0.5100 - val_acc: 0.8170\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s 87us/sample - loss: 0.1558 - acc: 0.9476 - val_loss: 0.5442 - val_acc: 0.8112\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 104us/sample - loss: 0.1498 - acc: 0.9502 - val_loss: 0.5683 - val_acc: 0.8069\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s 89us/sample - loss: 0.1438 - acc: 0.9528 - val_loss: 0.5873 - val_acc: 0.8071\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s 79us/sample - loss: 0.1376 - acc: 0.9571 - val_loss: 0.6233 - val_acc: 0.8019\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s 85us/sample - loss: 0.1321 - acc: 0.9573 - val_loss: 0.6360 - val_acc: 0.8042\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s 72us/sample - loss: 0.1277 - acc: 0.9601 - val_loss: 0.6338 - val_acc: 0.8068\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s 83us/sample - loss: 0.1223 - acc: 0.9617 - val_loss: 0.6414 - val_acc: 0.8062\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s 72us/sample - loss: 0.1180 - acc: 0.9648 - val_loss: 0.6554 - val_acc: 0.8041\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s 92us/sample - loss: 0.1136 - acc: 0.9668 - val_loss: 0.6939 - val_acc: 0.8029\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 103us/sample - loss: 0.1087 - acc: 0.9670 - val_loss: 0.7546 - val_acc: 0.7975\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s 100us/sample - loss: 0.1048 - acc: 0.9706 - val_loss: 0.7518 - val_acc: 0.7987\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 119us/sample - loss: 0.1026 - acc: 0.9704 - val_loss: 0.7613 - val_acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "his1 = model1.fit(X_train,y_train,epochs=20,batch_size=512,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,2,figsize=(14,6))\n",
    "ax[0].plot(his1.history['acc'],'r',label=\"Train data\")\n",
    "ax[0].plot(his1.history['val_acc'],'b',label=\"Validation data\")\n",
    "ax[0].set_title(\"Accurecy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
